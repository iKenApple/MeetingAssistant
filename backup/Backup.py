import json
import re
import threading

import pyaudio
import wave
import time
import subprocess
import numpy as np
import ast
import sys

import pygame
import requests
from cozepy import COZE_CN_BASE_URL
from cozepy import Coze, TokenAuth, Message, ChatStatus, MessageContentType  # noqa

from backup.VoiceprintRecognizer import VoiceprintRecognizer

from gtts import gTTS

# é…ç½®å‚æ•°
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 2048
SILENCE_DURATION = 2
THRESHOLD = 300
MAX_DURATION = 300

# Get an access_token through personal access token or oauth.
coze_api_token = 'pat_KOHb9a6TPJWs504SZYdrKdJNiG2a1JLZro9rtSkQHnzHkDl7ViPEBPa64Tiiovjg'
# The default access is api.coze.com, but if you need to access api.coze.cn,
# please use base_url to configure the api endpoint to access
coze_api_base = COZE_CN_BASE_URL

# Init the Coze client through the access_token.
coze = Coze(auth=TokenAuth(token=coze_api_token), base_url=coze_api_base)

# Create a workflow instance in Coze, copy the last number from the web link as the workflow's ID.
# ä¼šè®®è®°å½•å·¥ä½œæµ
workflow_id_meeting_record = '7478326457301008394'
workflow_id_meeting_guest = '7480057361940725795'
workflow_id_meeting_summary = '7478981906383781942'

# åˆå§‹åŒ–pygame
pygame.mixer.init()

def check_microphone():
    """æ£€æŸ¥éº¦å…‹é£æ˜¯å¦æ­£å¸¸å·¥ä½œ"""
    audio = pyaudio.PyAudio()
    try:
        if audio.get_device_count() == 0:
            raise Exception("æœªæ£€æµ‹åˆ°éŸ³é¢‘è¾“å…¥è®¾å¤‡")

        stream = audio.open(format=FORMAT, channels=CHANNELS,
                            rate=RATE, input=True,
                            frames_per_buffer=CHUNK,
                            start=False)

        try:
            stream.start_stream()
            for _ in range(3):  # å°è¯•è¯»å–ä¸‰æ¬¡
                data = stream.read(CHUNK, exception_on_overflow=False)
                if len(data) == 0:
                    raise Exception("æ— æ³•ä»éº¦å…‹é£è¯»å–æ•°æ®")
                if calculate_rms(data) > 1000:  # æ£€æµ‹æ˜æ˜¾ç¯å¢ƒå™ªéŸ³
                    raise Exception("æ£€æµ‹åˆ°å¼ºçƒˆèƒŒæ™¯å™ªéŸ³ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒ")
        finally:
            stream.stop_stream()
            stream.close()

    except Exception as e:
        print(f"éº¦å…‹é£æ£€æŸ¥å¤±è´¥: {str(e)}")
        print("è¯·æ£€æŸ¥ï¼š1.éº¦å…‹é£æƒé™ 2.è®¾å¤‡è¿æ¥ 3.èƒŒæ™¯å™ªéŸ³")
        sys.exit(1)
    finally:
        audio.terminate()

def calculate_rms(data):
    """å®‰å…¨è®¡ç®—éŸ³é¢‘èƒ½é‡å€¼"""
    audio_data = np.frombuffer(data, dtype=np.int16)
    if audio_data.size == 0:
        return 0.0

    with np.errstate(divide='ignore', invalid='ignore'):
        audio_data = audio_data.astype(np.float64)
        mean_square = np.mean(audio_data ** 2)
        if mean_square <= 0:
            return 0.0
        rms = np.sqrt(mean_square)

    return rms if not np.isnan(rms) else 0.0

def record_audio():
    """æ‰§è¡Œå½•éŸ³æ“ä½œ"""
    audio = pyaudio.PyAudio()
    stream = audio.open(format=FORMAT, channels=CHANNELS,
                        rate=RATE, input=True,
                        frames_per_buffer=CHUNK)

    #print("\nğŸ¤ è¯­éŸ³æ£€æµ‹ä¸­...ï¼ˆè¯´ã€Œå¼€å§‹ã€å”¤é†’ï¼‰")
    frames = []
    recording = False
    last_active = time.time()
    has_voice = False

    try:
        while True:
            data = stream.read(CHUNK, exception_on_overflow=False)
            rms = calculate_rms(data)

            # è¯­éŸ³æ´»åŠ¨æ£€æµ‹
            if rms > THRESHOLD:
                if not recording:
                    print("\nğŸ™ï¸ æ£€æµ‹åˆ°è¯­éŸ³ï¼Œå¼€å§‹å½•éŸ³...")
                    recording = True
                    frames = []  # ä¸¢å¼ƒä¹‹å‰çš„é™éŸ³æ•°æ®
                last_active = time.time()
                has_voice = True
                frames.append(data)
            elif recording:
                # åœ¨å½•éŸ³çŠ¶æ€ä¸‹å¤„ç†é™éŸ³
                frames.append(data)
                if time.time() - last_active > SILENCE_DURATION:
                    print("\nğŸ”‡ æŒç»­é™éŸ³ï¼Œåœæ­¢å½•éŸ³")
                    break

                if (time.time() - last_active) > MAX_DURATION:
                    print("\nâ° å·²è¾¾æœ€é•¿å½•éŸ³æ—¶é•¿")
                    break

    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­å½•éŸ³")
        return None
    finally:
        stream.stop_stream()
        stream.close()
        audio.terminate()

    if not has_voice:
        print("\nâŒ æœªæ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³")
        return None

    # ä¿å­˜å½•éŸ³æ–‡ä»¶
    filename = f"recordings/recording_{int(time.time())}.wav"
    with wave.open(filename, 'wb') as wf:
        wf.setnchannels(CHANNELS)
        wf.setsampwidth(pyaudio.get_sample_size(FORMAT))
        wf.setframerate(RATE)
        wf.writeframes(b''.join(frames))
    return filename

def speech_to_text(filename, result):
    """è°ƒç”¨è¯­éŸ³è¯†åˆ«å¼•æ“"""
    try:
        print(f"\nğŸ” æ­£åœ¨è¯†åˆ«: {filename}")
        cmd = f"wenet --language chinese {filename}"
        output = subprocess.run(
            cmd,
            shell=True,
            text=True,
            capture_output=True,
            timeout=15
        )
        output_str = output.stdout.strip()

        # æ¸…ç†è¾“å‡ºï¼ˆå¦‚æœåŒ…å«éå­—å…¸å†…å®¹ï¼‰
        dict_match = re.search(r"\{.*?\}", output_str)
        if not dict_match:
            raise ValueError("è¾“å‡ºä¸­æœªæ‰¾åˆ°æœ‰æ•ˆå­—å…¸")

        output_dict = ast.literal_eval(dict_match.group())
        result['text'] = output_dict.get("text")
    except subprocess.TimeoutExpired:
        print("è¯†åˆ«è¶…æ—¶ï¼Œè¯·æ£€æŸ¥weneté…ç½®")
        result['text'] = None
    except Exception as e:
        print(f"è¯†åˆ«å¤±è´¥: {str(e)}")
        result['text'] = None

def identify_speaker(filename, vr, result):
    """è°ƒç”¨å£°çº¹è¯†åˆ«å¼•æ“"""
    try:
        speaker, score = vr.identify_speaker(filename)
        result['speaker'] = speaker
        result['score'] = score
    except Exception as e:
        print(f"å£°çº¹è¯†åˆ«å¤±è´¥: {str(e)}")
        result['speaker'] = None
        result['score'] = None

def save_meeting_record(content, result_by_ai):
    # å‘é€ç»™cozeå·¥ä½œæµï¼Œåšä¼šè®®è®°å½•
    coze.workflows.runs.create(
        workflow_id=workflow_id_meeting_record,
        parameters={"input": content}
    )

def meeting_summary(content, result_by_ai):
    # å‘é€ç»™cozeå·¥ä½œæµï¼Œåšä¼šè®®è®°å½•
    workflow = coze.workflows.runs.create(
        workflow_id=workflow_id_meeting_summary,
        parameters={"input": content}
    )
    parsed_data = json.loads(workflow.data)
    result_by_ai['output'] = f'å°U: {parsed_data["output"]}'
    result_by_ai['query'] = parsed_data['query']
    result_by_ai['url'] = parsed_data["url"]

    print(parsed_data)
    if result_by_ai['query'] == 1:
        print("meeting_summary:")
        print(result_by_ai['output'])
        print("=" * 50)
        tts = gTTS(text=result_by_ai['output'], lang='zh-cn')
        # å°†æ–‡æœ¬è½¬ä¸ºè¯­éŸ³å¹¶ä¿å­˜ä¸ºéŸ³é¢‘æ–‡ä»¶
        tts.save("recordings/summary.mp3")
        # åŠ è½½MP3æ–‡ä»¶
        pygame.mixer.music.load("recordings/summary.mp3")

        # æ’­æ”¾MP3æ–‡ä»¶
        pygame.mixer.music.play()

        # ç­‰å¾…éŸ³ä¹æ’­æ”¾å®Œæ¯•
        while pygame.mixer.music.get_busy():
            pygame.time.Clock().tick(10)


def get_ai_response(content, result_by_ai):
    # å‘é€ç»™cozeå·¥ä½œæµï¼Œå˜‰å®¾å‘è¨€
    workflow = coze.workflows.runs.create(
        workflow_id=workflow_id_meeting_guest,
        parameters={"input": content}
    )
    parsed_data = json.loads(workflow.data)

    result_by_ai['output'] = f'å°U: {parsed_data["output"]}'
    result_by_ai['url'] = parsed_data["url"]

    if 'å°Uæ”¶åˆ°' not in parsed_data["output"]:
        print("get_ai_response:")
        print(result_by_ai['output'])
        print("=" * 50)
        #  æ’­æ”¾å°Uå‘è¨€
        response = requests.get(result_by_ai["url"])
        with open("recordings/ai.mp3", 'wb') as file:
            file.write(response.content)

        # åŠ è½½MP3æ–‡ä»¶
        pygame.mixer.music.load("recordings/ai.mp3")

        # æ’­æ”¾MP3æ–‡ä»¶
        pygame.mixer.music.play()

        # ç­‰å¾…éŸ³ä¹æ’­æ”¾å®Œæ¯•
        while pygame.mixer.music.get_busy():
            pygame.time.Clock().tick(10)

if __name__ == "__main__":
    vr = VoiceprintRecognizer()

    check_microphone()

    while True:
        try:
            filename = record_audio()
            if filename:
                start_time = time.time()

                # åˆ›å»ºå­—å…¸å­˜å‚¨ç»“æœ
                result = {'speaker': None, 'score': None, 'text': None}
                result_by_ai = {'output': None, 'url': None, 'query': None}

                # åˆ›å»ºçº¿ç¨‹
                threads = [
                    threading.Thread(target=identify_speaker, args=(filename, vr, result)),
                    threading.Thread(target=speech_to_text, args=(filename, result)),
                ]
                # å¯åŠ¨çº¿ç¨‹
                for thread in threads:
                    thread.start()

                # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
                for thread in threads:
                    thread.join()

                end_time = time.time()

                # è®¡ç®—è€—æ—¶
                execution_time = end_time - start_time
                print(f"è¯†åˆ«å˜‰å®¾å’Œè¯­éŸ³è½¬æ–‡å­—è€—æ—¶ï¼š{execution_time:.6f} ç§’")

                # æ‰“å°ç»“æœ
                speaker = result['speaker']
                score = result['score']
                text = result['text']

                if result['speaker'] != None and result['score'] != None:
                    if text:
                        print("\nğŸ“ è¯†åˆ«ç»“æœ:")
                        print("=" * 50)
                        print(f'{speaker}({score}): {text}')
                        print("=" * 50)

                    start_time_tmp = time.time()
                    # å‘é€ç»™cozeå·¥ä½œæµï¼Œåšä¼šè®®è®°å½•
                    #save_meeting_record(f'{speaker}({score}): {text}')
                    content = f'{speaker}({score}): {text}'
                    threading.Thread(target=save_meeting_record, args=(content, result_by_ai)).start()

                    # ä¼šè®®æ€»ç»“
                    thread_summary = threading.Thread(target=meeting_summary, args=(content, result_by_ai))
                    thread_summary.start()
                    thread_summary.join()

                    end_time = time.time()

                    # è®¡ç®—è€—æ—¶
                    execution_time = end_time - start_time_tmp
                    print(f"å°Uæ€»ç»“è€—æ—¶ï¼š{execution_time:.6f} ç§’")

                    # å‘é€ç»™cozeå·¥ä½œæµï¼Œå˜‰å®¾å‘è¨€
                    thread_response = threading.Thread(target=get_ai_response, args=(content,result_by_ai))
                    thread_response.start()
                    thread_response.join()

                    end_time = time.time()

                    # è®¡ç®—è€—æ—¶
                    execution_time = end_time - start_time_tmp
                    print(f"å˜‰å®¾å‘è¨€è€—æ—¶ï¼š{execution_time:.6f} ç§’")

                end_time = time.time()

                # è®¡ç®—è€—æ—¶
                execution_time = end_time - start_time
                print(f"å•æ¬¡å…¨æµç¨‹è€—æ—¶ï¼š{execution_time:.6f} ç§’")

        except Exception as e:
            print(f"å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            time.sleep(1)
            print("é‡æ–°å¼€å§‹è®°å½•...")